{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312031e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# importing data set into dataframes with two columns: Text and Class\n",
    "testData = pd.read_csv(\"/home/jovyan/binder/test.csv\", names=[\"Review\", \"Class\"], delimiter=\",\", header=None)\n",
    "trainData = pd.read_csv(\"/home/jovyan/binder/train.csv\", names=[\"Review\", \"Class\"], delimiter=\",\", header=None)\n",
    "valData = pd.read_csv(\"/home/jovyan/binder/val.csv\", names=[\"Review\", \"Class\"], delimiter=\",\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5def32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Class\n",
      "0  wild things is a suspenseful thriller starring...      1\n",
      "1  i know it already opened in december , but i f...      1\n",
      "2  what's shocking about \" carlito's way \" is how...      1\n",
      "3  uncompromising french director robert bresson'...      1\n",
      "4  aggressive , bleak , and unrelenting film abou...      1\n",
      "\n",
      "                                              Review  Class\n",
      "0  note : some may consider portions of the follo...      1\n",
      "1  note : some may consider portions of the follo...      1\n",
      "2  every once in a while you see a film that is s...      1\n",
      "3  when i was growing up in 1970s , boys in my sc...      1\n",
      "4  the muppet movie is the first , and the best m...      1\n",
      "\n",
      "                                              Review  Class\n",
      "0  if he doesn=92t watch out , mel gibson is in d...      1\n",
      "1  wong kar-wei's \" fallen angels \" is , on a pur...      1\n",
      "2  there is nothing like american history x in th...      1\n",
      "3  an unhappy italian housewife , a lonely waiter...      1\n",
      "4  when people are talking about good old times ,...      1\n",
      "\n",
      "Test Samples per class: [200 200]\n",
      "Train Samples per class: [700 700]\n",
      "Val Samples per class: [100 100]\n"
     ]
    }
   ],
   "source": [
    "print(testData.head())\n",
    "print(\"\")\n",
    "print(trainData.head())\n",
    "print(\"\")\n",
    "print(valData.head())\n",
    "print(\"\")\n",
    "print(\"Test Samples per class: {}\".format(np.bincount(testData.Class)))\n",
    "print(\"Train Samples per class: {}\".format(np.bincount(trainData.Class)))\n",
    "print(\"Val Samples per class: {}\".format(np.bincount(valData.Class)))\n",
    "# Count of samples in each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12020728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /srv/conda/envs/notebook\n",
      "\n",
      "  added / updated specs:\n",
      "    - nltk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2020.10.14 |                0         128 KB  anaconda\n",
      "    certifi-2020.6.20          |           py36_0         160 KB  anaconda\n",
      "    click-7.1.2                |             py_0          67 KB  anaconda\n",
      "    nltk-3.5                   |             py_0         1.1 MB  anaconda\n",
      "    regex-2020.10.15           |   py36h7b6447c_0         361 KB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  click              anaconda/noarch::click-7.1.2-py_0\n",
      "  nltk               anaconda/noarch::nltk-3.5-py_0\n",
      "  regex              anaconda/linux-64::regex-2020.10.15-py36h7b6447c_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2021.5.3~ --> anaconda::ca-certificates-2020.10.14-0\n",
      "  certifi            conda-forge::certifi-2021.5.30-py36h5~ --> anaconda::certifi-2020.6.20-py36_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi-2020.6.20    | 160 KB    | ##################################### | 100% \n",
      "ca-certificates-2020 | 128 KB    | ##################################### | 100% \n",
      "regex-2020.10.15     | 361 KB    | ##################################### | 100% \n",
      "nltk-3.5             | 1.1 MB    | ##################################### | 100% \n",
      "click-7.1.2          | 67 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c anaconda nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c381aeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "663e68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used for text cleaning of input data\n",
    "def clean(df):\n",
    "    corpus = list()  # define empty list for corpus\n",
    "    lines = df[\"Review\"].values.tolist()  # apply text values from \"Review\" column to the data frame\n",
    "    for text in lines: \n",
    "        text = text.lower() \n",
    "        text = re.sub(r\"[,.\\\"!$%^&*(){}?/;`~:<>+=-]\", \"\", text)  # regexp used to remove all special characters\n",
    "        tokens = word_tokenize(text)  # splitting text\n",
    "        table = str.maketrans('', '', string.punctuation) \n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        words = ' '.join(words)  # joining tokenize words together\n",
    "        corpus.append(words)  # amends cleaned text to corpus\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c40b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying clean function to data sets\n",
    "clTest = clean(testData)\n",
    "clTrain = clean(trainData)\n",
    "clVal = clean(valData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ac7e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading TF-IDF class for feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TF = TfidfVectorizer(min_df=15) \n",
    "xTrain = TF.fit_transform(clTrain).toarray() \n",
    "yTrain = trainData[['Class']].values\n",
    "xTest = TF.transform(clTest).toarray()\n",
    "yTest = testData[['Class']].values\n",
    "xVal = TF.transform(clVal).toarray()\n",
    "yVal = valData[['Class']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11dc0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Multinomial Naive Bayes model for text classification\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mNB = MultinomialNB()\n",
    "mNB.fit(xTrain, np.ravel(yTrain)) \n",
    "y_pred_ts = mNB.predict(xTest)\n",
    "y_pred_tr = mNB.predict(xTrain)\n",
    "y_pred_va = mNB.predict(xVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbb991b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.86       200\n",
      "           1       0.87      0.82      0.85       200\n",
      "\n",
      "    accuracy                           0.85       400\n",
      "   macro avg       0.85      0.85      0.85       400\n",
      "weighted avg       0.85      0.85      0.85       400\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[176  24]\n",
      " [ 35 165]]\n",
      "\n",
      "Sentiment Analysis on Test Set:\n",
      "[1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Validation Set Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.81       100\n",
      "           1       0.81      0.80      0.80       100\n",
      "\n",
      "    accuracy                           0.81       200\n",
      "   macro avg       0.81      0.81      0.80       200\n",
      "weighted avg       0.81      0.81      0.80       200\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[81 19]\n",
      " [20 80]]\n",
      "\n",
      "Sentiment Analysis on Validation Set:\n",
      "[1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1\n",
      " 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
      " 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# classification report used to evaluate perfomance (Accuracy) of ML model on test and val datasets\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Test Set Metrics:\\n{}\".format(classification_report(yTest, y_pred_ts)))\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\\n{}\".format(confusion_matrix(yTest, y_pred_ts)))\n",
    "print(\"\")\n",
    "print(\"Sentiment Analysis on Test Set:\\n{}\".format(mNB.predict(TF.transform(clTest).toarray())))\n",
    "print(\"\")\n",
    "print(\"Validation Set Metrics:\\n{}\".format(classification_report(yVal, y_pred_va)))\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\\n{}\".format(confusion_matrix(yVal, y_pred_va)))\n",
    "print(\"\")\n",
    "print(\"Sentiment Analysis on Validation Set:\\n{}\".format(mNB.predict(TF.transform(clVal).toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ccf092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
